{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from Bio import SeqIO\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, Dropout, MaxPooling1D, Flatten, Dense\n",
    "import os\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set seeds for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Set deterministic operations in TensorFlow\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9499, 33)\n",
      "(9499,)\n",
      "(3226, 33)\n",
      "(3226,)\n"
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "r_test_x = []\n",
    "r_test_y = []\n",
    "posit_1 = 1;\n",
    "negat_0 = 0;\n",
    "win_size = 33 # actual window size\n",
    "win_size_kernel = int(win_size/2 + 1)\n",
    "\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# TRAIN DATASET -------------------------------------------------------------\n",
    "#for positive sequence\n",
    "def inner1():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #rint(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    train_x.append(integer_encoded)\n",
    "    train_y.append(posit_1)\n",
    "for seq_record in SeqIO.parse(\"data/train/fasta/positive_sites.fasta\", \"fasta\"): # training data positive\n",
    "    inner1()\n",
    "#for negative sequence\n",
    "def inner2():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #print(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    train_x.append(integer_encoded)\n",
    "    train_y.append(negat_0)\n",
    "for seq_record in SeqIO.parse(\"data/train/fasta/negative_sites.fasta\", \"fasta\"): # training data negative\n",
    "    inner2()\n",
    "# Changing to array (matrix)    \n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "\n",
    "#-------------------------TEST DATASET----------------------------------------\n",
    "#for positive sequence\n",
    "def innertest1():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #rint(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded)\n",
    "    r_test_y.append(posit_1)\n",
    "for seq_record in SeqIO.parse(\"data/test/fasta/test_positive_sites.fasta\", \"fasta\"): # test positive\n",
    "    innertest1()\n",
    "#for negative sequence\n",
    "def innertest2():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #print(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded)\n",
    "    r_test_y.append(negat_0)\n",
    "for seq_record in SeqIO.parse(\"data/test/fasta/test_negative_sites.fasta\", \"fasta\"): # test negative\n",
    "    innertest2()\n",
    "\n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "\n",
    "\n",
    "# epochs = 100\n",
    "# num_classes = 2\n",
    "# batch_size = 256\n",
    "# optimize_2 = tf.keras.optimizers.Adam()\n",
    "\n",
    "# loss_2 = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "# test_size = 0.2\n",
    "seed = 3\n",
    "\n",
    "X_train = train_x\n",
    "y_train = train_y\n",
    "\n",
    "X_test = r_test_x\n",
    "y_test = r_test_y\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9499, 1024)\n",
      "(9499,)\n",
      "(3226, 1024)\n",
      "(3226,)\n"
     ]
    }
   ],
   "source": [
    "train_positive_pt5 = pd.read_csv(\"../data/train/features/train_positive_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "train_negative_pt5 = pd.read_csv(\"../data/train/features/train_negative_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "test_positive_pt5 = pd.read_csv(\"../data/test/features/test_positive_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "test_negative_pt5 = pd.read_csv(\"../data/test/features/test_negative_ProtT5-XL-UniRef50.csv\", header = None).iloc[:,2:]\n",
    "\n",
    "\n",
    "# create labels\n",
    "train_positive_labels = np.ones(train_positive_pt5.shape[0])\n",
    "train_negative_labels = np.zeros(train_negative_pt5.shape[0])\n",
    "test_positive_labels = np.ones(test_positive_pt5.shape[0])\n",
    "test_negative_labels = np.zeros(test_negative_pt5.shape[0])\n",
    "\n",
    "# stack positive and negative data together\n",
    "X_train_pt5 = np.vstack((train_positive_pt5,train_negative_pt5))\n",
    "X_test_pt5 = np.vstack((test_positive_pt5,test_negative_pt5))\n",
    "y_train_pt5 = np.concatenate((train_positive_labels, train_negative_labels), axis = 0)\n",
    "y_test_pt5 = np.concatenate((test_positive_labels, test_negative_labels), axis = 0)\n",
    "\n",
    "# # shuffle X and y together\n",
    "# X_train_pt5, y_train_pt5 = shuffle(X_train_pt5, y_train_pt5)\n",
    "# X_test_pt5, y_test_pt5 = shuffle(X_test_pt5, y_test_pt5)\n",
    "print(X_train_pt5.shape)\n",
    "print(y_train_pt5.shape)\n",
    "print(X_test_pt5.shape)\n",
    "print(y_test_pt5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 0 1 1 1]\n",
      "[0. 0. 0. 1. 1. 0. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the split ratio and random state for reproducibility\n",
    "split_ratio = 0.1\n",
    "random_state = 42\n",
    "\n",
    "# Perform the split on the smaller dataset (X_train_pt5, y_train_pt5)\n",
    "X_train_pt5, X_val_pt5, y_train_pt5, y_val_pt5 = train_test_split(\n",
    "    X_train_pt5, y_train_pt5, test_size=split_ratio, random_state=random_state, stratify=y_train_pt5)\n",
    "\n",
    "# Use the same indices to split the larger dataset (X_train, y_train)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=split_ratio, random_state=random_state, stratify=y_train)\n",
    "\n",
    "# Now, both X_val_pt5 and X_val correspond to the same split in y_train_pt5 and y_train\n",
    "print( y_val[:10])\n",
    "print( y_val_pt5[:10])\n",
    "\n",
    "# X_test = X_test[:2000]\n",
    "# y_test = y_test[:2000]\n",
    "# X_test_pt5 = X_test_pt5[:2000]\n",
    "# y_test_pt5 = y_test_pt5[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    # learning curves of model accuracy\n",
    "    plt.plot(history.history['accuracy'], label='train_acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "def evaluate_model(model, X_val, X_val_pt5, y_val):\n",
    "    y_true = y_val\n",
    "    # Predict probabilities (or logits if using `from_logits=True`).\n",
    "    y_pred_probs = model.predict([X_val, X_val_pt5])\n",
    "\n",
    "    # Convert probabilities/logits to binary predictions (threshold = 0.5).\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # If y_true is one-hot encoded, convert it to binary format\n",
    "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:  # Check if y_true is one-hot encoded\n",
    "        y_true = np.argmax(y_true, axis=1)  # Convert one-hot encoded y_true to binary labels\n",
    "\n",
    "    # Ensure y_pred is also 1D\n",
    "    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)  # Convert y_pred to binary labels if necessary\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Compute Specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'MCC: {mcc}')\n",
    "    print(f'AUC: {auc}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Specificity: {specificity}')\n",
    "    print(f'F1: {f1}')\n",
    "\n",
    "    return accuracy, mcc, auc, precision, recall, specificity, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m conv1d_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mconv1d_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:694\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m    691\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    698\u001b[0m     )\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 2)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, Dropout, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "def conv1d_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(256, 21, input_length=33))\n",
    "    \n",
    "    # Conv1D and Dropout layers\n",
    "    model.add(Conv1D(64, kernel_size=17, activation='relu', kernel_initializer='he_normal', padding='valid'))\n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    model.add(Conv1D(128, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    # MaxPooling, Flatten, and Dense layers\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # model.add(Dense(768, activation='relu', kernel_initializer='he_normal'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    \n",
    "    # model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "conv1d_model = conv1d_model()\n",
    "conv1d_model.summary()\n",
    "\n",
    "# Fit the model\n",
    "history = conv1d_model.fit(X_train, y_train, epochs=100, batch_size=256, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_24\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_24\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_23 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_93 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_85 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_94 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_86 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_95 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_87 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_96 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">689,282</span> (2.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m689,282\u001b[0m (2.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">689,282</span> (2.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m689,282\u001b[0m (2.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5062 - loss: 0.7058\n",
      "Epoch 2/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5525 - loss: 0.6895\n",
      "Epoch 3/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5852 - loss: 0.6765\n",
      "Epoch 4/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6159 - loss: 0.6627\n",
      "Epoch 5/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6496 - loss: 0.6407\n",
      "Epoch 6/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6560 - loss: 0.6249\n",
      "Epoch 7/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6811 - loss: 0.6091\n",
      "Epoch 8/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6912 - loss: 0.5913\n",
      "Epoch 9/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7039 - loss: 0.5823\n",
      "Epoch 10/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7159 - loss: 0.5665\n",
      "Epoch 11/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7248 - loss: 0.5547\n",
      "Epoch 12/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7269 - loss: 0.5430\n",
      "Epoch 13/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7299 - loss: 0.5387\n",
      "Epoch 14/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7424 - loss: 0.5300\n",
      "Epoch 15/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7504 - loss: 0.5220\n",
      "Epoch 16/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7567 - loss: 0.5077\n",
      "Epoch 17/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7557 - loss: 0.5089\n",
      "Epoch 18/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7633 - loss: 0.4952\n",
      "Epoch 19/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7757 - loss: 0.4801\n",
      "Epoch 20/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7774 - loss: 0.4722\n",
      "Epoch 21/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7758 - loss: 0.4740\n",
      "Epoch 22/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7821 - loss: 0.4682\n",
      "Epoch 23/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7971 - loss: 0.4475\n",
      "Epoch 24/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7902 - loss: 0.4430\n",
      "Epoch 25/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8123 - loss: 0.4268\n",
      "Epoch 26/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8142 - loss: 0.4181\n",
      "Epoch 27/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8237 - loss: 0.4095\n",
      "Epoch 28/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8219 - loss: 0.4010\n",
      "Epoch 29/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8241 - loss: 0.4002\n",
      "Epoch 30/30\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8328 - loss: 0.3877\n"
     ]
    }
   ],
   "source": [
    "def ANN():\n",
    "    ann_input = Input(shape=(1024,))\n",
    "\n",
    "    # Hidden layers\n",
    "    x = Dense(512, activation='relu')(ann_input)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    ann_output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    ann_model = Model(ann_input, ann_output)\n",
    "\n",
    "    # Compile the model\n",
    "    ann_model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "    return ann_model\n",
    "\n",
    "ann_model = ANN()\n",
    "ann_model.summary()\n",
    "\n",
    "# Fit the model\n",
    "history = ann_model.fit(X_train_pt5, y_train_pt5, epochs=30, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_71\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_71\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_142     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_71        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │ input_layer_142[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_389 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,392</span> │ embedding_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_124   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_389[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_71          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_12… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_143     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_302 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,184</span> │ flatten_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_304 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,800</span> │ input_layer_143[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_195         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_302[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_196         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_304[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_303 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │ dropout_195[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_305 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │ dropout_196[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_124     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_303[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_305[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_306 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ concatenate_124[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_142     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_71        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m21\u001b[0m)    │      \u001b[38;5;34m1,344\u001b[0m │ input_layer_142[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_389 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m3,392\u001b[0m │ embedding_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_124   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d_389[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_71          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m448\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_12… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_143     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_302 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m7,184\u001b[0m │ flatten_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_304 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m32,800\u001b[0m │ input_layer_143[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_195         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_302[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_196         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_304[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_303 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m136\u001b[0m │ dropout_195[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_305 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m264\u001b[0m │ dropout_196[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_124     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_303[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_305[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_306 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ concatenate_124[\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,137</span> (176.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,137\u001b[0m (176.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,137</span> (176.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m45,137\u001b[0m (176.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Concatenate, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "def round_layer(x):\n",
    "    return tf.math.round(x)  # Custom layer to round values to the nearest integers\n",
    "\n",
    "def build_simple_model(input_shape_conv, input_shape_ann):\n",
    "    # Conv1D branch for sequence data\n",
    "    conv_input = Input(shape=input_shape_conv)\n",
    "\n",
    "    # Embedding layer\n",
    "    x_conv = Embedding(input_dim=64, output_dim=21, input_length=input_shape_conv[0])(conv_input)\n",
    "\n",
    "    # Simple Conv1D + MaxPooling layer\n",
    "    x_conv = Conv1D(32, kernel_size=5, activation='relu', kernel_initializer='he_normal')(x_conv)\n",
    "    x_conv = MaxPooling1D(pool_size=2)(x_conv)\n",
    "    x_conv = Flatten()(x_conv)\n",
    "\n",
    "    # Simple Dense layer for sequence features with integer output\n",
    "    x_conv = Dense(16, activation='relu', kernel_initializer='he_normal')(x_conv)\n",
    "    x_conv = Dropout(0.2)(x_conv)\n",
    "    x_conv = Dense(8, activation='relu', kernel_initializer='he_normal')(x_conv)  # Final integer size\n",
    "\n",
    "    # ANN branch for prot_t5 embeddings\n",
    "    ann_input = Input(shape=(input_shape_ann,))\n",
    "\n",
    "    # Simple Dense layer for ANN features with integer output\n",
    "    x_ann = Dense(32, activation='relu')(ann_input)\n",
    "    x_ann = Dropout(0.2)(x_ann)\n",
    "    x_ann = Dense(8, activation='relu')(x_ann)  # Final integer size\n",
    "\n",
    "\n",
    "    # Concatenate Conv1D and ANN branches\n",
    "    combined = Concatenate()([x_conv, x_ann])\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[conv_input, ann_input], outputs=output_layer)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss=BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the model with Conv1D input shape (33,) and ANN input shape 1024\n",
    "simple_model = build_simple_model((33,), 1024)\n",
    "simple_model.summary()\n",
    "\n",
    "class_weight = {0: 1.0, 1: 2.0}\n",
    "\n",
    "# Fit the simple model\n",
    "# history = simple_model.fit([X_train, X_train_pt5], y_train, epochs=100, batch_size=64,\n",
    "#                                  verbose=1, \n",
    "#                                  validation_data=([X_test, X_test_pt5], y_test)\n",
    "#                                 )\n",
    "\n",
    "# plot(history)\n",
    "\n",
    "# simple_model.load_weights('models/simple_model.weights.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_70\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_70\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_140     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_70        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │ input_layer_140[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_122   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_382 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │ embedding_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_383 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ embedding_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_384 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,392</span> │ embedding_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_385 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,736</span> │ embedding_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_386 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,080</span> │ embedding_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_387 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,424</span> │ embedding_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_388 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │ max_pooling1d_12… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_122     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_382[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ conv1d_383[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_384[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_385[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_386[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_387[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ conv1d_388[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_123   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_122[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_70          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_12… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_141     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_298 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">57,360</span> │ flatten_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_299 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │ input_layer_141[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_192         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_298[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_193         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_299[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_123     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_192[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_193[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_301 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span> │ concatenate_123[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_140     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_70        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m21\u001b[0m)    │      \u001b[38;5;34m1,344\u001b[0m │ input_layer_140[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_122   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m21\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ embedding_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_382 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │        \u001b[38;5;34m704\u001b[0m │ embedding_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_383 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m2,048\u001b[0m │ embedding_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_384 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m3,392\u001b[0m │ embedding_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_385 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m4,736\u001b[0m │ embedding_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_386 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m6,080\u001b[0m │ embedding_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_387 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m7,424\u001b[0m │ embedding_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_388 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │        \u001b[38;5;34m704\u001b[0m │ max_pooling1d_12… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_122     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m224\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv1d_382[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ conv1d_383[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_384[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_385[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_386[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_387[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ conv1d_388[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_123   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m224\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ concatenate_122[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_70          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_12… │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_141     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_298 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │     \u001b[38;5;34m57,360\u001b[0m │ flatten_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_299 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m65,600\u001b[0m │ input_layer_141[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_192         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_298[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_193         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_299[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_123     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_192[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_193[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_301 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m81\u001b[0m │ concatenate_123[\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">149,473</span> (583.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m149,473\u001b[0m (583.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">149,473</span> (583.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m149,473\u001b[0m (583.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Concatenate, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "def inception_module(input_tensor):\n",
    "    # Branch 1: Conv1D with kernel size 1\n",
    "    branch1 = Conv1D(32, kernel_size=1, activation='relu', padding='same')(input_tensor)\n",
    "\n",
    "    # Branch 2: Conv1D with kernel size 3\n",
    "    branch2 = Conv1D(32, kernel_size=3, activation='relu', padding='same')(input_tensor)\n",
    "\n",
    "    # Branch 3: Conv1D with kernel size 5\n",
    "    branch3 = Conv1D(32, kernel_size=5, activation='relu', padding='same')(input_tensor)\n",
    "\n",
    "    # Branch 4: Conv1D with kernel size 7\n",
    "    branch4 = Conv1D(32, kernel_size=7, activation='relu', padding='same')(input_tensor)\n",
    "\n",
    "    # Branch 5: Conv1D with kernel size 9\n",
    "    branch5 = Conv1D(32, kernel_size=9, activation='relu', padding='same')(input_tensor)\n",
    "\n",
    "    # Branch 6: Conv1D with kernel size 11\n",
    "    branch6 = Conv1D(32, kernel_size=11, activation='relu', padding='same')(input_tensor)\n",
    "\n",
    "    # Branch 7: MaxPooling + Conv1D with kernel size 1\n",
    "    branch7 = MaxPooling1D(pool_size=3, strides=1, padding='same')(input_tensor)\n",
    "    branch7 = Conv1D(32, kernel_size=1, activation='relu', padding='same')(branch7)\n",
    "\n",
    "    # Concatenate all branches\n",
    "    output = Concatenate()([branch1, branch2, branch3, branch4, branch5, branch6, branch7])\n",
    "    return output\n",
    "\n",
    "def build_inception_model(input_shape_conv, input_shape_ann):\n",
    "    # Conv1D branch for sequence data\n",
    "    conv_input = Input(shape=input_shape_conv)\n",
    "\n",
    "    # Embedding layer\n",
    "    x_conv = Embedding(input_dim=64, output_dim=21, input_length=input_shape_conv[0])(conv_input)\n",
    "\n",
    "    # Apply Inception module instead of Conv1D\n",
    "    x_conv = inception_module(x_conv)\n",
    "\n",
    "    # MaxPooling and Flatten layers\n",
    "    x_conv = MaxPooling1D(pool_size=2)(x_conv)\n",
    "    x_conv = Flatten()(x_conv)\n",
    "\n",
    "    # Dense layer for sequence features\n",
    "    x_conv = Dense(16, activation='relu')(x_conv)\n",
    "    x_conv = Dropout(0.3)(x_conv)\n",
    "\n",
    "    # ANN branch for prot_t5 embeddings\n",
    "    ann_input = Input(shape=(input_shape_ann,))\n",
    "\n",
    "    # Simple Dense layer for ANN features\n",
    "    x_ann = Dense(64, activation='relu')(ann_input)\n",
    "    x_ann = Dropout(0.3)(x_ann)\n",
    "\n",
    "    # Concatenate Conv1D (Inception) and ANN branches\n",
    "    combined = Concatenate()([x_conv, x_ann])\n",
    "\n",
    "    # Output layer\n",
    "    x = Dense(16, activation='relu')(combined)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[conv_input, ann_input], outputs=output_layer)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss=BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the model with Conv1D input shape (33,) and ANN input shape 1024\n",
    "inception_model = build_inception_model((33,), 1024)\n",
    "inception_model.summary()\n",
    "\n",
    "# # early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# # Fit the Inception model\n",
    "# history = inception_model.fit([X_train, X_train_pt5], y_train, epochs=1000, batch_size=64, verbose=1, callbacks=[early_stopping]\n",
    "#                               , validation_data=([X_val, X_val_pt5], y_val))\n",
    "\n",
    "\n",
    "\n",
    "plot(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/101\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 99ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 28 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Accuracy: 0.7625542467451953\n",
      "MCC: 0.3325323587003211\n",
      "AUC: 0.7789605793378882\n",
      "Precision: 0.2202835332606325\n",
      "Recall: 0.7984189723320159\n",
      "Specificity: 0.7595021863437605\n",
      "F1: 0.3452991452991453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7625542467451953,\n",
       " 0.3325323587003211,\n",
       " 0.7789605793378882,\n",
       " 0.2202835332606325,\n",
       " 0.7984189723320159,\n",
       " 0.7595021863437605,\n",
       " 0.3452991452991453)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.load_weights('models/simple_model_full_train.weights.h5')\n",
    "\n",
    "# evaluate_model(inception_model, X_val, X_val_pt5, y_val)\n",
    "\n",
    "evaluate_model(simple_model, X_test, X_test_pt5, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1/10...\n",
      "Epoch 1/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5367 - loss: 0.6881 - val_accuracy: 0.6516 - val_loss: 0.6476\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6514 - loss: 0.6355 - val_accuracy: 0.6716 - val_loss: 0.6192\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6882 - loss: 0.6026 - val_accuracy: 0.6863 - val_loss: 0.5972\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7055 - loss: 0.5734 - val_accuracy: 0.7021 - val_loss: 0.5779\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7290 - loss: 0.5499 - val_accuracy: 0.7095 - val_loss: 0.5657\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5283 - val_accuracy: 0.7158 - val_loss: 0.5559\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7506 - loss: 0.5146 - val_accuracy: 0.7189 - val_loss: 0.5531\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7545 - loss: 0.5043 - val_accuracy: 0.7211 - val_loss: 0.5466\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7577 - loss: 0.4980 - val_accuracy: 0.7253 - val_loss: 0.5450\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7648 - loss: 0.4871 - val_accuracy: 0.7263 - val_loss: 0.5418\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7758 - loss: 0.4772 - val_accuracy: 0.7326 - val_loss: 0.5394\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7826 - loss: 0.4678 - val_accuracy: 0.7316 - val_loss: 0.5349\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7852 - loss: 0.4625 - val_accuracy: 0.7347 - val_loss: 0.5347\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7834 - loss: 0.4593 - val_accuracy: 0.7316 - val_loss: 0.5314\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7899 - loss: 0.4459 - val_accuracy: 0.7379 - val_loss: 0.5296\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7889 - loss: 0.4434 - val_accuracy: 0.7379 - val_loss: 0.5268\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7948 - loss: 0.4404 - val_accuracy: 0.7400 - val_loss: 0.5246\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8035 - loss: 0.4295 - val_accuracy: 0.7421 - val_loss: 0.5221\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8071 - loss: 0.4245 - val_accuracy: 0.7453 - val_loss: 0.5213\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8140 - loss: 0.4198 - val_accuracy: 0.7442 - val_loss: 0.5187\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8123 - loss: 0.4158 - val_accuracy: 0.7442 - val_loss: 0.5183\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8171 - loss: 0.4058 - val_accuracy: 0.7484 - val_loss: 0.5174\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8202 - loss: 0.4002 - val_accuracy: 0.7505 - val_loss: 0.5127\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8287 - loss: 0.3940 - val_accuracy: 0.7505 - val_loss: 0.5137\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8301 - loss: 0.3902 - val_accuracy: 0.7516 - val_loss: 0.5116\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8314 - loss: 0.3808 - val_accuracy: 0.7558 - val_loss: 0.5103\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8387 - loss: 0.3762 - val_accuracy: 0.7558 - val_loss: 0.5089\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8419 - loss: 0.3693 - val_accuracy: 0.7537 - val_loss: 0.5095\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8456 - loss: 0.3610 - val_accuracy: 0.7558 - val_loss: 0.5113\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8415 - loss: 0.3650 - val_accuracy: 0.7547 - val_loss: 0.5084\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8519 - loss: 0.3528 - val_accuracy: 0.7558 - val_loss: 0.5059\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8610 - loss: 0.3463 - val_accuracy: 0.7558 - val_loss: 0.5049\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8636 - loss: 0.3411 - val_accuracy: 0.7611 - val_loss: 0.5042\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8642 - loss: 0.3338 - val_accuracy: 0.7611 - val_loss: 0.5022\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8697 - loss: 0.3283 - val_accuracy: 0.7632 - val_loss: 0.5025\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8726 - loss: 0.3231 - val_accuracy: 0.7642 - val_loss: 0.5013\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8722 - loss: 0.3202 - val_accuracy: 0.7621 - val_loss: 0.4995\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8796 - loss: 0.3120 - val_accuracy: 0.7642 - val_loss: 0.4984\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8848 - loss: 0.3066 - val_accuracy: 0.7632 - val_loss: 0.5021\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8910 - loss: 0.2991 - val_accuracy: 0.7653 - val_loss: 0.5020\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8851 - loss: 0.2974 - val_accuracy: 0.7653 - val_loss: 0.4974\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8912 - loss: 0.2916 - val_accuracy: 0.7632 - val_loss: 0.5002\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8927 - loss: 0.2839 - val_accuracy: 0.7600 - val_loss: 0.5014\n",
      "Epoch 44/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8969 - loss: 0.2795 - val_accuracy: 0.7621 - val_loss: 0.5005\n",
      "Epoch 45/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9002 - loss: 0.2738 - val_accuracy: 0.7642 - val_loss: 0.5003\n",
      "Epoch 46/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9004 - loss: 0.2681 - val_accuracy: 0.7663 - val_loss: 0.5046\n",
      "Epoch 47/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9054 - loss: 0.2608 - val_accuracy: 0.7621 - val_loss: 0.5052\n",
      "Epoch 48/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9078 - loss: 0.2574 - val_accuracy: 0.7716 - val_loss: 0.5016\n",
      "Epoch 49/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9071 - loss: 0.2534 - val_accuracy: 0.7716 - val_loss: 0.5008\n",
      "Epoch 50/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9144 - loss: 0.2461 - val_accuracy: 0.7705 - val_loss: 0.5040\n",
      "Epoch 51/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9181 - loss: 0.2400 - val_accuracy: 0.7716 - val_loss: 0.5051\n",
      "Epoch 52/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9215 - loss: 0.2338 - val_accuracy: 0.7695 - val_loss: 0.5079\n",
      "Epoch 53/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9165 - loss: 0.2338 - val_accuracy: 0.7705 - val_loss: 0.5102\n",
      "Epoch 54/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9249 - loss: 0.2259 - val_accuracy: 0.7737 - val_loss: 0.5120\n",
      "Epoch 55/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9238 - loss: 0.2240 - val_accuracy: 0.7705 - val_loss: 0.5152\n",
      "Epoch 56/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9310 - loss: 0.2146 - val_accuracy: 0.7726 - val_loss: 0.5131\n",
      "Epoch 57/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9350 - loss: 0.2042 - val_accuracy: 0.7758 - val_loss: 0.5149\n",
      "Epoch 58/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9344 - loss: 0.2056 - val_accuracy: 0.7768 - val_loss: 0.5170\n",
      "Epoch 59/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9349 - loss: 0.2024 - val_accuracy: 0.7789 - val_loss: 0.5175\n",
      "Epoch 60/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9376 - loss: 0.1934 - val_accuracy: 0.7737 - val_loss: 0.5212\n",
      "Epoch 61/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9375 - loss: 0.1934 - val_accuracy: 0.7747 - val_loss: 0.5213\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Accuracy: 0.7652631578947369\n",
      "MCC: 0.5339672693983346\n",
      "AUC: 0.7653795695493387\n",
      "Precision: 0.7381404174573055\n",
      "Recall: 0.820675105485232\n",
      "Specificity: 0.7100840336134454\n",
      "F1: 0.7772227772227772\n",
      "Training on fold 2/10...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5566 - loss: 0.6819 - val_accuracy: 0.6179 - val_loss: 0.6524\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6532 - loss: 0.6298 - val_accuracy: 0.6526 - val_loss: 0.6253\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6876 - loss: 0.5976 - val_accuracy: 0.6768 - val_loss: 0.6064\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7127 - loss: 0.5718 - val_accuracy: 0.6874 - val_loss: 0.5913\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7293 - loss: 0.5491 - val_accuracy: 0.7042 - val_loss: 0.5755\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7423 - loss: 0.5310 - val_accuracy: 0.7084 - val_loss: 0.5652\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7515 - loss: 0.5181 - val_accuracy: 0.7137 - val_loss: 0.5573\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7565 - loss: 0.5061 - val_accuracy: 0.7189 - val_loss: 0.5502\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7657 - loss: 0.4934 - val_accuracy: 0.7189 - val_loss: 0.5459\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7681 - loss: 0.4850 - val_accuracy: 0.7253 - val_loss: 0.5434\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7740 - loss: 0.4786 - val_accuracy: 0.7211 - val_loss: 0.5385\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7766 - loss: 0.4699 - val_accuracy: 0.7316 - val_loss: 0.5356\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7838 - loss: 0.4632 - val_accuracy: 0.7305 - val_loss: 0.5330\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7896 - loss: 0.4563 - val_accuracy: 0.7316 - val_loss: 0.5324\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7947 - loss: 0.4482 - val_accuracy: 0.7316 - val_loss: 0.5292\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8022 - loss: 0.4414 - val_accuracy: 0.7305 - val_loss: 0.5257\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8050 - loss: 0.4331 - val_accuracy: 0.7379 - val_loss: 0.5245\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8042 - loss: 0.4309 - val_accuracy: 0.7368 - val_loss: 0.5217\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8082 - loss: 0.4244 - val_accuracy: 0.7453 - val_loss: 0.5217\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8221 - loss: 0.4151 - val_accuracy: 0.7432 - val_loss: 0.5200\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8190 - loss: 0.4126 - val_accuracy: 0.7463 - val_loss: 0.5186\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8264 - loss: 0.4057 - val_accuracy: 0.7463 - val_loss: 0.5164\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8237 - loss: 0.3983 - val_accuracy: 0.7463 - val_loss: 0.5153\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8300 - loss: 0.3929 - val_accuracy: 0.7516 - val_loss: 0.5147\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8344 - loss: 0.3868 - val_accuracy: 0.7537 - val_loss: 0.5139\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8373 - loss: 0.3785 - val_accuracy: 0.7537 - val_loss: 0.5112\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8439 - loss: 0.3719 - val_accuracy: 0.7589 - val_loss: 0.5089\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8456 - loss: 0.3682 - val_accuracy: 0.7589 - val_loss: 0.5092\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8481 - loss: 0.3641 - val_accuracy: 0.7611 - val_loss: 0.5083\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8529 - loss: 0.3564 - val_accuracy: 0.7621 - val_loss: 0.5055\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8615 - loss: 0.3478 - val_accuracy: 0.7663 - val_loss: 0.5058\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8603 - loss: 0.3424 - val_accuracy: 0.7674 - val_loss: 0.5049\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8632 - loss: 0.3349 - val_accuracy: 0.7642 - val_loss: 0.5038\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8672 - loss: 0.3289 - val_accuracy: 0.7632 - val_loss: 0.5022\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8742 - loss: 0.3245 - val_accuracy: 0.7663 - val_loss: 0.5032\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8742 - loss: 0.3184 - val_accuracy: 0.7611 - val_loss: 0.5020\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8739 - loss: 0.3132 - val_accuracy: 0.7716 - val_loss: 0.5035\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8805 - loss: 0.3036 - val_accuracy: 0.7726 - val_loss: 0.5015\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8887 - loss: 0.2945 - val_accuracy: 0.7726 - val_loss: 0.5016\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8883 - loss: 0.2888 - val_accuracy: 0.7716 - val_loss: 0.5071\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8911 - loss: 0.2854 - val_accuracy: 0.7695 - val_loss: 0.5059\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8961 - loss: 0.2762 - val_accuracy: 0.7695 - val_loss: 0.5035\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8977 - loss: 0.2748 - val_accuracy: 0.7716 - val_loss: 0.5037\n",
      "Epoch 44/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8934 - loss: 0.2709 - val_accuracy: 0.7716 - val_loss: 0.5039\n",
      "Epoch 45/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9041 - loss: 0.2615 - val_accuracy: 0.7737 - val_loss: 0.5035\n",
      "Epoch 46/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9121 - loss: 0.2506 - val_accuracy: 0.7716 - val_loss: 0.5082\n",
      "Epoch 47/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9095 - loss: 0.2475 - val_accuracy: 0.7705 - val_loss: 0.5078\n",
      "Epoch 48/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9190 - loss: 0.2394 - val_accuracy: 0.7716 - val_loss: 0.5115\n",
      "Epoch 49/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9158 - loss: 0.2375 - val_accuracy: 0.7726 - val_loss: 0.5109\n",
      "Epoch 50/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9173 - loss: 0.2307 - val_accuracy: 0.7716 - val_loss: 0.5139\n",
      "Epoch 51/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9205 - loss: 0.2251 - val_accuracy: 0.7705 - val_loss: 0.5121\n",
      "Epoch 52/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9247 - loss: 0.2205 - val_accuracy: 0.7726 - val_loss: 0.5196\n",
      "Epoch 53/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9290 - loss: 0.2123 - val_accuracy: 0.7705 - val_loss: 0.5187\n",
      "Epoch 54/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9331 - loss: 0.2068 - val_accuracy: 0.7705 - val_loss: 0.5205\n",
      "Epoch 55/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9351 - loss: 0.2026 - val_accuracy: 0.7716 - val_loss: 0.5237\n",
      "Epoch 56/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9397 - loss: 0.1940 - val_accuracy: 0.7726 - val_loss: 0.5254\n",
      "Epoch 57/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9405 - loss: 0.1896 - val_accuracy: 0.7695 - val_loss: 0.5385\n",
      "Epoch 58/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9429 - loss: 0.1861 - val_accuracy: 0.7716 - val_loss: 0.5368\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Accuracy: 0.7726315789473684\n",
      "MCC: 0.5495071144159985\n",
      "AUC: 0.7731118882668689\n",
      "Precision: 0.7419354838709677\n",
      "Recall: 0.8301486199575372\n",
      "Specificity: 0.7160751565762005\n",
      "F1: 0.7835671342685371\n",
      "Training on fold 3/10...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5570 - loss: 0.6838 - val_accuracy: 0.6495 - val_loss: 0.6450\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6600 - loss: 0.6355 - val_accuracy: 0.6737 - val_loss: 0.6135\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6911 - loss: 0.6007 - val_accuracy: 0.6916 - val_loss: 0.5919\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7093 - loss: 0.5757 - val_accuracy: 0.7021 - val_loss: 0.5750\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7261 - loss: 0.5538 - val_accuracy: 0.7032 - val_loss: 0.5599\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7354 - loss: 0.5349 - val_accuracy: 0.7168 - val_loss: 0.5480\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7547 - loss: 0.5189 - val_accuracy: 0.7211 - val_loss: 0.5410\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7559 - loss: 0.5060 - val_accuracy: 0.7284 - val_loss: 0.5351\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7710 - loss: 0.4942 - val_accuracy: 0.7295 - val_loss: 0.5307\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7739 - loss: 0.4839 - val_accuracy: 0.7358 - val_loss: 0.5271\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7743 - loss: 0.4791 - val_accuracy: 0.7358 - val_loss: 0.5244\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7809 - loss: 0.4724 - val_accuracy: 0.7389 - val_loss: 0.5209\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7848 - loss: 0.4638 - val_accuracy: 0.7368 - val_loss: 0.5183\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7947 - loss: 0.4584 - val_accuracy: 0.7432 - val_loss: 0.5152\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8019 - loss: 0.4517 - val_accuracy: 0.7453 - val_loss: 0.5125\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7988 - loss: 0.4477 - val_accuracy: 0.7463 - val_loss: 0.5093\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8048 - loss: 0.4372 - val_accuracy: 0.7505 - val_loss: 0.5068\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8098 - loss: 0.4323 - val_accuracy: 0.7505 - val_loss: 0.5038\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8118 - loss: 0.4234 - val_accuracy: 0.7579 - val_loss: 0.5007\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8146 - loss: 0.4227 - val_accuracy: 0.7621 - val_loss: 0.4991\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8206 - loss: 0.4147 - val_accuracy: 0.7589 - val_loss: 0.4959\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8256 - loss: 0.4091 - val_accuracy: 0.7632 - val_loss: 0.4932\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8277 - loss: 0.4024 - val_accuracy: 0.7611 - val_loss: 0.4903\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8330 - loss: 0.3996 - val_accuracy: 0.7621 - val_loss: 0.4885\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8313 - loss: 0.3945 - val_accuracy: 0.7600 - val_loss: 0.4867\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8415 - loss: 0.3827 - val_accuracy: 0.7653 - val_loss: 0.4855\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8412 - loss: 0.3814 - val_accuracy: 0.7653 - val_loss: 0.4832\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8444 - loss: 0.3781 - val_accuracy: 0.7705 - val_loss: 0.4816\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8424 - loss: 0.3729 - val_accuracy: 0.7684 - val_loss: 0.4796\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8494 - loss: 0.3655 - val_accuracy: 0.7716 - val_loss: 0.4794\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8549 - loss: 0.3584 - val_accuracy: 0.7758 - val_loss: 0.4767\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8532 - loss: 0.3565 - val_accuracy: 0.7737 - val_loss: 0.4748\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8589 - loss: 0.3541 - val_accuracy: 0.7747 - val_loss: 0.4739\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8568 - loss: 0.3465 - val_accuracy: 0.7747 - val_loss: 0.4715\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8619 - loss: 0.3370 - val_accuracy: 0.7726 - val_loss: 0.4732\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8598 - loss: 0.3354 - val_accuracy: 0.7726 - val_loss: 0.4714\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8639 - loss: 0.3321 - val_accuracy: 0.7758 - val_loss: 0.4695\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8714 - loss: 0.3289 - val_accuracy: 0.7768 - val_loss: 0.4687\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8681 - loss: 0.3224 - val_accuracy: 0.7737 - val_loss: 0.4707\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8718 - loss: 0.3192 - val_accuracy: 0.7768 - val_loss: 0.4686\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8811 - loss: 0.3134 - val_accuracy: 0.7832 - val_loss: 0.4660\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8757 - loss: 0.3091 - val_accuracy: 0.7821 - val_loss: 0.4690\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8778 - loss: 0.3037 - val_accuracy: 0.7811 - val_loss: 0.4663\n",
      "Epoch 44/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8820 - loss: 0.2991 - val_accuracy: 0.7832 - val_loss: 0.4666\n",
      "Epoch 45/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8820 - loss: 0.2927 - val_accuracy: 0.7874 - val_loss: 0.4673\n",
      "Epoch 46/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8858 - loss: 0.2858 - val_accuracy: 0.7842 - val_loss: 0.4669\n",
      "Epoch 47/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8911 - loss: 0.2865 - val_accuracy: 0.7821 - val_loss: 0.4674\n",
      "Epoch 48/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8929 - loss: 0.2803 - val_accuracy: 0.7832 - val_loss: 0.4663\n",
      "Epoch 49/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8885 - loss: 0.2814 - val_accuracy: 0.7832 - val_loss: 0.4669\n",
      "Epoch 50/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8951 - loss: 0.2724 - val_accuracy: 0.7800 - val_loss: 0.4674\n",
      "Epoch 51/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8994 - loss: 0.2623 - val_accuracy: 0.7821 - val_loss: 0.4696\n",
      "Epoch 52/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9055 - loss: 0.2625 - val_accuracy: 0.7821 - val_loss: 0.4697\n",
      "Epoch 53/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9039 - loss: 0.2589 - val_accuracy: 0.7811 - val_loss: 0.4702\n",
      "Epoch 54/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9027 - loss: 0.2539 - val_accuracy: 0.7800 - val_loss: 0.4705\n",
      "Epoch 55/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9067 - loss: 0.2515 - val_accuracy: 0.7800 - val_loss: 0.4715\n",
      "Epoch 56/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9082 - loss: 0.2445 - val_accuracy: 0.7842 - val_loss: 0.4721\n",
      "Epoch 57/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9153 - loss: 0.2452 - val_accuracy: 0.7789 - val_loss: 0.4712\n",
      "Epoch 58/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9149 - loss: 0.2373 - val_accuracy: 0.7811 - val_loss: 0.4716\n",
      "Epoch 59/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9180 - loss: 0.2299 - val_accuracy: 0.7811 - val_loss: 0.4740\n",
      "Epoch 60/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9174 - loss: 0.2337 - val_accuracy: 0.7768 - val_loss: 0.4745\n",
      "Epoch 61/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9178 - loss: 0.2276 - val_accuracy: 0.7811 - val_loss: 0.4753\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Accuracy: 0.783157894736842\n",
      "MCC: 0.5673260202885196\n",
      "AUC: 0.7830210840302986\n",
      "Precision: 0.7672583826429981\n",
      "Recall: 0.8155136268343816\n",
      "Specificity: 0.7505285412262156\n",
      "F1: 0.790650406504065\n",
      "Training on fold 4/10...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5269 - loss: 0.6901 - val_accuracy: 0.6368 - val_loss: 0.6460\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6480 - loss: 0.6395 - val_accuracy: 0.6621 - val_loss: 0.6165\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6867 - loss: 0.6092 - val_accuracy: 0.6863 - val_loss: 0.5940\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7037 - loss: 0.5850 - val_accuracy: 0.7074 - val_loss: 0.5755\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7166 - loss: 0.5633 - val_accuracy: 0.7221 - val_loss: 0.5571\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7413 - loss: 0.5399 - val_accuracy: 0.7263 - val_loss: 0.5415\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7485 - loss: 0.5238 - val_accuracy: 0.7400 - val_loss: 0.5309\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7506 - loss: 0.5106 - val_accuracy: 0.7442 - val_loss: 0.5231\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7560 - loss: 0.5028 - val_accuracy: 0.7442 - val_loss: 0.5179\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7610 - loss: 0.4929 - val_accuracy: 0.7505 - val_loss: 0.5131\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7691 - loss: 0.4858 - val_accuracy: 0.7568 - val_loss: 0.5080\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7715 - loss: 0.4795 - val_accuracy: 0.7600 - val_loss: 0.5038\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7795 - loss: 0.4715 - val_accuracy: 0.7600 - val_loss: 0.5007\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7846 - loss: 0.4651 - val_accuracy: 0.7632 - val_loss: 0.4966\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7907 - loss: 0.4574 - val_accuracy: 0.7663 - val_loss: 0.4948\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7932 - loss: 0.4522 - val_accuracy: 0.7695 - val_loss: 0.4900\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7955 - loss: 0.4445 - val_accuracy: 0.7705 - val_loss: 0.4876\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8008 - loss: 0.4372 - val_accuracy: 0.7716 - val_loss: 0.4843\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8066 - loss: 0.4311 - val_accuracy: 0.7726 - val_loss: 0.4812\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8077 - loss: 0.4234 - val_accuracy: 0.7716 - val_loss: 0.4781\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8178 - loss: 0.4188 - val_accuracy: 0.7737 - val_loss: 0.4757\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8178 - loss: 0.4086 - val_accuracy: 0.7758 - val_loss: 0.4738\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8210 - loss: 0.4035 - val_accuracy: 0.7789 - val_loss: 0.4707\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8290 - loss: 0.3983 - val_accuracy: 0.7768 - val_loss: 0.4690\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8261 - loss: 0.3947 - val_accuracy: 0.7789 - val_loss: 0.4666\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8318 - loss: 0.3864 - val_accuracy: 0.7811 - val_loss: 0.4654\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8289 - loss: 0.3830 - val_accuracy: 0.7842 - val_loss: 0.4641\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8471 - loss: 0.3718 - val_accuracy: 0.7842 - val_loss: 0.4621\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8482 - loss: 0.3686 - val_accuracy: 0.7874 - val_loss: 0.4635\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8472 - loss: 0.3639 - val_accuracy: 0.7853 - val_loss: 0.4592\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8497 - loss: 0.3548 - val_accuracy: 0.7863 - val_loss: 0.4575\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8540 - loss: 0.3506 - val_accuracy: 0.7853 - val_loss: 0.4557\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8597 - loss: 0.3445 - val_accuracy: 0.7811 - val_loss: 0.4579\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8592 - loss: 0.3406 - val_accuracy: 0.7800 - val_loss: 0.4545\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8674 - loss: 0.3312 - val_accuracy: 0.7832 - val_loss: 0.4543\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8651 - loss: 0.3257 - val_accuracy: 0.7747 - val_loss: 0.4553\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8752 - loss: 0.3205 - val_accuracy: 0.7800 - val_loss: 0.4536\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8793 - loss: 0.3091 - val_accuracy: 0.7821 - val_loss: 0.4527\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8785 - loss: 0.3129 - val_accuracy: 0.7768 - val_loss: 0.4554\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8749 - loss: 0.3026 - val_accuracy: 0.7853 - val_loss: 0.4531\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8861 - loss: 0.2966 - val_accuracy: 0.7789 - val_loss: 0.4546\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8825 - loss: 0.2939 - val_accuracy: 0.7842 - val_loss: 0.4539\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8882 - loss: 0.2862 - val_accuracy: 0.7811 - val_loss: 0.4557\n",
      "Epoch 44/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8901 - loss: 0.2822 - val_accuracy: 0.7863 - val_loss: 0.4548\n",
      "Epoch 45/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8959 - loss: 0.2726 - val_accuracy: 0.7842 - val_loss: 0.4557\n",
      "Epoch 46/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8939 - loss: 0.2725 - val_accuracy: 0.7811 - val_loss: 0.4567\n",
      "Epoch 47/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9019 - loss: 0.2632 - val_accuracy: 0.7874 - val_loss: 0.4552\n",
      "Epoch 48/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9020 - loss: 0.2622 - val_accuracy: 0.7895 - val_loss: 0.4565\n",
      "Epoch 49/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9034 - loss: 0.2554 - val_accuracy: 0.7832 - val_loss: 0.4600\n",
      "Epoch 50/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9061 - loss: 0.2526 - val_accuracy: 0.7832 - val_loss: 0.4614\n",
      "Epoch 51/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9124 - loss: 0.2453 - val_accuracy: 0.7832 - val_loss: 0.4600\n",
      "Epoch 52/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9145 - loss: 0.2388 - val_accuracy: 0.7842 - val_loss: 0.4623\n",
      "Epoch 53/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9127 - loss: 0.2340 - val_accuracy: 0.7874 - val_loss: 0.4656\n",
      "Epoch 54/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9205 - loss: 0.2259 - val_accuracy: 0.7874 - val_loss: 0.4667\n",
      "Epoch 55/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9214 - loss: 0.2221 - val_accuracy: 0.7853 - val_loss: 0.4683\n",
      "Epoch 56/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9298 - loss: 0.2126 - val_accuracy: 0.7874 - val_loss: 0.4702\n",
      "Epoch 57/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9263 - loss: 0.2146 - val_accuracy: 0.7884 - val_loss: 0.4729\n",
      "Epoch 58/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9338 - loss: 0.2030 - val_accuracy: 0.7895 - val_loss: 0.4740\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Accuracy: 0.7821052631578947\n",
      "MCC: 0.5619430564884272\n",
      "AUC: 0.7807556511324527\n",
      "Precision: 0.79296875\n",
      "Recall: 0.8007889546351085\n",
      "Specificity: 0.7607223476297968\n",
      "F1: 0.7968596663395485\n",
      "Training on fold 5/10...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5440 - loss: 0.6858 - val_accuracy: 0.6537 - val_loss: 0.6449\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6688 - loss: 0.6314 - val_accuracy: 0.6642 - val_loss: 0.6168\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6945 - loss: 0.6002 - val_accuracy: 0.6821 - val_loss: 0.5975\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7176 - loss: 0.5726 - val_accuracy: 0.6863 - val_loss: 0.5812\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7244 - loss: 0.5489 - val_accuracy: 0.6979 - val_loss: 0.5683\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7371 - loss: 0.5293 - val_accuracy: 0.7116 - val_loss: 0.5596\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7480 - loss: 0.5172 - val_accuracy: 0.7179 - val_loss: 0.5539\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7564 - loss: 0.4990 - val_accuracy: 0.7253 - val_loss: 0.5489\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7658 - loss: 0.4886 - val_accuracy: 0.7295 - val_loss: 0.5455\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7695 - loss: 0.4757 - val_accuracy: 0.7442 - val_loss: 0.5411\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7758 - loss: 0.4719 - val_accuracy: 0.7453 - val_loss: 0.5377\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7814 - loss: 0.4657 - val_accuracy: 0.7505 - val_loss: 0.5349\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7813 - loss: 0.4592 - val_accuracy: 0.7516 - val_loss: 0.5325\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7856 - loss: 0.4485 - val_accuracy: 0.7526 - val_loss: 0.5296\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7957 - loss: 0.4390 - val_accuracy: 0.7526 - val_loss: 0.5264\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8032 - loss: 0.4336 - val_accuracy: 0.7484 - val_loss: 0.5236\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8051 - loss: 0.4260 - val_accuracy: 0.7516 - val_loss: 0.5217\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8147 - loss: 0.4177 - val_accuracy: 0.7537 - val_loss: 0.5188\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8124 - loss: 0.4129 - val_accuracy: 0.7558 - val_loss: 0.5170\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8197 - loss: 0.4027 - val_accuracy: 0.7526 - val_loss: 0.5147\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8221 - loss: 0.4004 - val_accuracy: 0.7537 - val_loss: 0.5126\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8225 - loss: 0.3918 - val_accuracy: 0.7537 - val_loss: 0.5106\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8314 - loss: 0.3830 - val_accuracy: 0.7547 - val_loss: 0.5089\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8355 - loss: 0.3779 - val_accuracy: 0.7558 - val_loss: 0.5088\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8400 - loss: 0.3743 - val_accuracy: 0.7558 - val_loss: 0.5073\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8404 - loss: 0.3666 - val_accuracy: 0.7558 - val_loss: 0.5057\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8506 - loss: 0.3524 - val_accuracy: 0.7579 - val_loss: 0.5054\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8468 - loss: 0.3497 - val_accuracy: 0.7579 - val_loss: 0.5053\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8523 - loss: 0.3470 - val_accuracy: 0.7589 - val_loss: 0.5038\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8543 - loss: 0.3430 - val_accuracy: 0.7579 - val_loss: 0.5031\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8537 - loss: 0.3352 - val_accuracy: 0.7642 - val_loss: 0.5035\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8582 - loss: 0.3343 - val_accuracy: 0.7579 - val_loss: 0.5026\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8689 - loss: 0.3252 - val_accuracy: 0.7600 - val_loss: 0.5038\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8703 - loss: 0.3175 - val_accuracy: 0.7600 - val_loss: 0.5036\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8717 - loss: 0.3122 - val_accuracy: 0.7653 - val_loss: 0.5049\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8776 - loss: 0.3076 - val_accuracy: 0.7589 - val_loss: 0.5058\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8805 - loss: 0.3031 - val_accuracy: 0.7663 - val_loss: 0.5050\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8781 - loss: 0.2969 - val_accuracy: 0.7684 - val_loss: 0.5062\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8862 - loss: 0.2960 - val_accuracy: 0.7684 - val_loss: 0.5084\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8906 - loss: 0.2869 - val_accuracy: 0.7674 - val_loss: 0.5086\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8870 - loss: 0.2819 - val_accuracy: 0.7695 - val_loss: 0.5099\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8918 - loss: 0.2759 - val_accuracy: 0.7674 - val_loss: 0.5123\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8997 - loss: 0.2714 - val_accuracy: 0.7674 - val_loss: 0.5116\n",
      "Epoch 44/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8985 - loss: 0.2627 - val_accuracy: 0.7653 - val_loss: 0.5132\n",
      "Epoch 45/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9015 - loss: 0.2605 - val_accuracy: 0.7663 - val_loss: 0.5178\n",
      "Epoch 46/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9055 - loss: 0.2567 - val_accuracy: 0.7663 - val_loss: 0.5175\n",
      "Epoch 47/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9099 - loss: 0.2480 - val_accuracy: 0.7663 - val_loss: 0.5169\n",
      "Epoch 48/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9106 - loss: 0.2459 - val_accuracy: 0.7684 - val_loss: 0.5195\n",
      "Epoch 49/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9074 - loss: 0.2446 - val_accuracy: 0.7684 - val_loss: 0.5208\n",
      "Epoch 50/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9193 - loss: 0.2342 - val_accuracy: 0.7642 - val_loss: 0.5260\n",
      "Epoch 51/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9176 - loss: 0.2286 - val_accuracy: 0.7663 - val_loss: 0.5288\n",
      "Epoch 52/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9247 - loss: 0.2267 - val_accuracy: 0.7726 - val_loss: 0.5294\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Accuracy: 0.7578947368421053\n",
      "MCC: 0.5153870164809963\n",
      "AUC: 0.7572611439544795\n",
      "Precision: 0.7544554455445545\n",
      "Recall: 0.7823408624229979\n",
      "Specificity: 0.7321814254859611\n",
      "F1: 0.7681451612903226\n",
      "Training on fold 6/10...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5503 - loss: 0.6853 - val_accuracy: 0.6358 - val_loss: 0.6423\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6565 - loss: 0.6318 - val_accuracy: 0.6632 - val_loss: 0.6108\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6988 - loss: 0.5962 - val_accuracy: 0.6821 - val_loss: 0.5895\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7223 - loss: 0.5726 - val_accuracy: 0.6926 - val_loss: 0.5719\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7365 - loss: 0.5480 - val_accuracy: 0.7095 - val_loss: 0.5574\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7420 - loss: 0.5288 - val_accuracy: 0.7179 - val_loss: 0.5449\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7555 - loss: 0.5110 - val_accuracy: 0.7274 - val_loss: 0.5365\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7561 - loss: 0.5020 - val_accuracy: 0.7326 - val_loss: 0.5309\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7672 - loss: 0.4872 - val_accuracy: 0.7368 - val_loss: 0.5261\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7755 - loss: 0.4809 - val_accuracy: 0.7400 - val_loss: 0.5224\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7794 - loss: 0.4736 - val_accuracy: 0.7432 - val_loss: 0.5190\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7861 - loss: 0.4596 - val_accuracy: 0.7463 - val_loss: 0.5147\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7863 - loss: 0.4555 - val_accuracy: 0.7484 - val_loss: 0.5124\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7913 - loss: 0.4517 - val_accuracy: 0.7526 - val_loss: 0.5095\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7923 - loss: 0.4441 - val_accuracy: 0.7463 - val_loss: 0.5068\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8069 - loss: 0.4331 - val_accuracy: 0.7537 - val_loss: 0.5070\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8101 - loss: 0.4285 - val_accuracy: 0.7505 - val_loss: 0.5040\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8110 - loss: 0.4187 - val_accuracy: 0.7505 - val_loss: 0.5027\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8158 - loss: 0.4152 - val_accuracy: 0.7537 - val_loss: 0.5008\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8151 - loss: 0.4087 - val_accuracy: 0.7537 - val_loss: 0.5003\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8179 - loss: 0.4046 - val_accuracy: 0.7600 - val_loss: 0.4961\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8219 - loss: 0.3965 - val_accuracy: 0.7663 - val_loss: 0.4963\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8343 - loss: 0.3878 - val_accuracy: 0.7663 - val_loss: 0.4946\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8373 - loss: 0.3844 - val_accuracy: 0.7695 - val_loss: 0.4946\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8389 - loss: 0.3756 - val_accuracy: 0.7653 - val_loss: 0.4934\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8374 - loss: 0.3741 - val_accuracy: 0.7716 - val_loss: 0.4906\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8440 - loss: 0.3680 - val_accuracy: 0.7716 - val_loss: 0.4927\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8503 - loss: 0.3595 - val_accuracy: 0.7695 - val_loss: 0.4923\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8570 - loss: 0.3521 - val_accuracy: 0.7737 - val_loss: 0.4912\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8569 - loss: 0.3459 - val_accuracy: 0.7747 - val_loss: 0.4917\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8565 - loss: 0.3464 - val_accuracy: 0.7747 - val_loss: 0.4924\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8599 - loss: 0.3374 - val_accuracy: 0.7768 - val_loss: 0.4916\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8650 - loss: 0.3300 - val_accuracy: 0.7747 - val_loss: 0.4890\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8671 - loss: 0.3282 - val_accuracy: 0.7695 - val_loss: 0.4911\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8727 - loss: 0.3238 - val_accuracy: 0.7695 - val_loss: 0.4891\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8740 - loss: 0.3157 - val_accuracy: 0.7716 - val_loss: 0.4896\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8734 - loss: 0.3075 - val_accuracy: 0.7726 - val_loss: 0.4901\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8817 - loss: 0.3034 - val_accuracy: 0.7716 - val_loss: 0.4904\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8837 - loss: 0.2995 - val_accuracy: 0.7684 - val_loss: 0.4901\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8903 - loss: 0.2913 - val_accuracy: 0.7674 - val_loss: 0.4929\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8937 - loss: 0.2854 - val_accuracy: 0.7684 - val_loss: 0.4926\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8918 - loss: 0.2797 - val_accuracy: 0.7663 - val_loss: 0.4939\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8975 - loss: 0.2775 - val_accuracy: 0.7695 - val_loss: 0.4936\n",
      "Epoch 44/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8965 - loss: 0.2750 - val_accuracy: 0.7695 - val_loss: 0.4936\n",
      "Epoch 45/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8997 - loss: 0.2682 - val_accuracy: 0.7695 - val_loss: 0.4971\n",
      "Epoch 46/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9000 - loss: 0.2606 - val_accuracy: 0.7684 - val_loss: 0.4981\n",
      "Epoch 47/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9072 - loss: 0.2557 - val_accuracy: 0.7705 - val_loss: 0.4968\n",
      "Epoch 48/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9066 - loss: 0.2550 - val_accuracy: 0.7642 - val_loss: 0.5034\n",
      "Epoch 49/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9087 - loss: 0.2442 - val_accuracy: 0.7695 - val_loss: 0.5003\n",
      "Epoch 50/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9166 - loss: 0.2387 - val_accuracy: 0.7674 - val_loss: 0.5047\n",
      "Epoch 51/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9168 - loss: 0.2375 - val_accuracy: 0.7695 - val_loss: 0.5040\n",
      "Epoch 52/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9150 - loss: 0.2323 - val_accuracy: 0.7737 - val_loss: 0.5061\n",
      "Epoch 53/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9226 - loss: 0.2279 - val_accuracy: 0.7705 - val_loss: 0.5096\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Accuracy: 0.7747368421052632\n",
      "MCC: 0.5545621457699237\n",
      "AUC: 0.7759228466910542\n",
      "Precision: 0.739961759082218\n",
      "Recall: 0.832258064516129\n",
      "Specificity: 0.7195876288659794\n",
      "F1: 0.7834008097165992\n",
      "Training on fold 7/10...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5348 - loss: 0.6908 - val_accuracy: 0.6653 - val_loss: 0.6414\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6511 - loss: 0.6359 - val_accuracy: 0.7011 - val_loss: 0.6016\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6903 - loss: 0.6009 - val_accuracy: 0.7221 - val_loss: 0.5744\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7100 - loss: 0.5758 - val_accuracy: 0.7347 - val_loss: 0.5545\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7165 - loss: 0.5536 - val_accuracy: 0.7347 - val_loss: 0.5382\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7385 - loss: 0.5326 - val_accuracy: 0.7379 - val_loss: 0.5274\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7474 - loss: 0.5208 - val_accuracy: 0.7400 - val_loss: 0.5184\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7544 - loss: 0.5067 - val_accuracy: 0.7411 - val_loss: 0.5138\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7604 - loss: 0.4966 - val_accuracy: 0.7474 - val_loss: 0.5097\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7696 - loss: 0.4848 - val_accuracy: 0.7526 - val_loss: 0.5055\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7809 - loss: 0.4771 - val_accuracy: 0.7526 - val_loss: 0.5015\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7823 - loss: 0.4670 - val_accuracy: 0.7547 - val_loss: 0.4987\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7858 - loss: 0.4587 - val_accuracy: 0.7589 - val_loss: 0.4953\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7921 - loss: 0.4527 - val_accuracy: 0.7579 - val_loss: 0.4931\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7997 - loss: 0.4476 - val_accuracy: 0.7589 - val_loss: 0.4902\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8085 - loss: 0.4361 - val_accuracy: 0.7600 - val_loss: 0.4871\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8084 - loss: 0.4297 - val_accuracy: 0.7684 - val_loss: 0.4833\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8105 - loss: 0.4236 - val_accuracy: 0.7632 - val_loss: 0.4822\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8155 - loss: 0.4165 - val_accuracy: 0.7695 - val_loss: 0.4780\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8246 - loss: 0.4120 - val_accuracy: 0.7747 - val_loss: 0.4750\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8234 - loss: 0.4014 - val_accuracy: 0.7747 - val_loss: 0.4743\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8283 - loss: 0.4027 - val_accuracy: 0.7758 - val_loss: 0.4715\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8260 - loss: 0.3955 - val_accuracy: 0.7747 - val_loss: 0.4705\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8348 - loss: 0.3846 - val_accuracy: 0.7726 - val_loss: 0.4692\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8438 - loss: 0.3760 - val_accuracy: 0.7758 - val_loss: 0.4693\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8458 - loss: 0.3729 - val_accuracy: 0.7747 - val_loss: 0.4695\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8468 - loss: 0.3681 - val_accuracy: 0.7800 - val_loss: 0.4636\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8499 - loss: 0.3625 - val_accuracy: 0.7832 - val_loss: 0.4623\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8548 - loss: 0.3545 - val_accuracy: 0.7884 - val_loss: 0.4627\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8609 - loss: 0.3460 - val_accuracy: 0.7863 - val_loss: 0.4626\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8606 - loss: 0.3395 - val_accuracy: 0.7863 - val_loss: 0.4620\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8650 - loss: 0.3363 - val_accuracy: 0.7895 - val_loss: 0.4603\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8622 - loss: 0.3356 - val_accuracy: 0.7905 - val_loss: 0.4602\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8666 - loss: 0.3264 - val_accuracy: 0.7884 - val_loss: 0.4616\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8724 - loss: 0.3221 - val_accuracy: 0.7926 - val_loss: 0.4601\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8804 - loss: 0.3134 - val_accuracy: 0.7947 - val_loss: 0.4601\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8761 - loss: 0.3114 - val_accuracy: 0.7989 - val_loss: 0.4599\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8840 - loss: 0.2998 - val_accuracy: 0.7947 - val_loss: 0.4589\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8837 - loss: 0.3003 - val_accuracy: 0.7989 - val_loss: 0.4609\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8861 - loss: 0.2913 - val_accuracy: 0.7958 - val_loss: 0.4641\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8877 - loss: 0.2886 - val_accuracy: 0.8021 - val_loss: 0.4616\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8916 - loss: 0.2831 - val_accuracy: 0.8000 - val_loss: 0.4617\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8977 - loss: 0.2751 - val_accuracy: 0.7989 - val_loss: 0.4612\n",
      "Epoch 44/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8959 - loss: 0.2717 - val_accuracy: 0.7968 - val_loss: 0.4622\n",
      "Epoch 45/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8997 - loss: 0.2663 - val_accuracy: 0.8021 - val_loss: 0.4650\n",
      "Epoch 46/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9020 - loss: 0.2616 - val_accuracy: 0.7979 - val_loss: 0.4640\n",
      "Epoch 47/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9075 - loss: 0.2529 - val_accuracy: 0.7989 - val_loss: 0.4677\n",
      "Epoch 48/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9129 - loss: 0.2466 - val_accuracy: 0.7989 - val_loss: 0.4687\n",
      "Epoch 49/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9130 - loss: 0.2485 - val_accuracy: 0.8000 - val_loss: 0.4669\n",
      "Epoch 50/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9144 - loss: 0.2405 - val_accuracy: 0.8032 - val_loss: 0.4693\n",
      "Epoch 51/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9174 - loss: 0.2322 - val_accuracy: 0.8084 - val_loss: 0.4719\n",
      "Epoch 52/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9177 - loss: 0.2288 - val_accuracy: 0.8042 - val_loss: 0.4738\n",
      "Epoch 53/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9230 - loss: 0.2227 - val_accuracy: 0.8095 - val_loss: 0.4732\n",
      "Epoch 54/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9277 - loss: 0.2182 - val_accuracy: 0.8063 - val_loss: 0.4728\n",
      "Epoch 55/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9280 - loss: 0.2161 - val_accuracy: 0.8032 - val_loss: 0.4746\n",
      "Epoch 56/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9325 - loss: 0.2074 - val_accuracy: 0.8063 - val_loss: 0.4770\n",
      "Epoch 57/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9297 - loss: 0.2051 - val_accuracy: 0.8021 - val_loss: 0.4793\n",
      "Epoch 58/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9330 - loss: 0.1985 - val_accuracy: 0.8021 - val_loss: 0.4840\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Accuracy: 0.7947368421052632\n",
      "MCC: 0.5926068663712484\n",
      "AUC: 0.7958226882407211\n",
      "Precision: 0.7643564356435644\n",
      "Recall: 0.8354978354978355\n",
      "Specificity: 0.7561475409836066\n",
      "F1: 0.7983453981385729\n",
      "Training on fold 8/10...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5588 - loss: 0.6806 - val_accuracy: 0.6463 - val_loss: 0.6437\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6548 - loss: 0.6295 - val_accuracy: 0.6800 - val_loss: 0.6110\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6907 - loss: 0.5978 - val_accuracy: 0.7042 - val_loss: 0.5885\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7120 - loss: 0.5733 - val_accuracy: 0.7105 - val_loss: 0.5702\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7289 - loss: 0.5519 - val_accuracy: 0.7179 - val_loss: 0.5546\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7328 - loss: 0.5383 - val_accuracy: 0.7295 - val_loss: 0.5394\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7470 - loss: 0.5229 - val_accuracy: 0.7432 - val_loss: 0.5271\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7549 - loss: 0.5085 - val_accuracy: 0.7432 - val_loss: 0.5183\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7566 - loss: 0.4970 - val_accuracy: 0.7474 - val_loss: 0.5102\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7704 - loss: 0.4916 - val_accuracy: 0.7516 - val_loss: 0.5054\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7780 - loss: 0.4817 - val_accuracy: 0.7547 - val_loss: 0.5016\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7818 - loss: 0.4695 - val_accuracy: 0.7537 - val_loss: 0.4989\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7832 - loss: 0.4636 - val_accuracy: 0.7568 - val_loss: 0.4941\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7856 - loss: 0.4605 - val_accuracy: 0.7600 - val_loss: 0.4911\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7937 - loss: 0.4485 - val_accuracy: 0.7621 - val_loss: 0.4872\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7955 - loss: 0.4457 - val_accuracy: 0.7589 - val_loss: 0.4864\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8013 - loss: 0.4375 - val_accuracy: 0.7568 - val_loss: 0.4849\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8040 - loss: 0.4307 - val_accuracy: 0.7621 - val_loss: 0.4807\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8107 - loss: 0.4267 - val_accuracy: 0.7653 - val_loss: 0.4779\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8131 - loss: 0.4194 - val_accuracy: 0.7684 - val_loss: 0.4761\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8134 - loss: 0.4154 - val_accuracy: 0.7663 - val_loss: 0.4739\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8208 - loss: 0.4086 - val_accuracy: 0.7726 - val_loss: 0.4713\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8220 - loss: 0.4040 - val_accuracy: 0.7737 - val_loss: 0.4704\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8255 - loss: 0.4023 - val_accuracy: 0.7747 - val_loss: 0.4679\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8293 - loss: 0.3916 - val_accuracy: 0.7737 - val_loss: 0.4668\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8351 - loss: 0.3863 - val_accuracy: 0.7747 - val_loss: 0.4660\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8369 - loss: 0.3808 - val_accuracy: 0.7811 - val_loss: 0.4655\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8408 - loss: 0.3751 - val_accuracy: 0.7779 - val_loss: 0.4632\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8401 - loss: 0.3732 - val_accuracy: 0.7832 - val_loss: 0.4619\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8448 - loss: 0.3643 - val_accuracy: 0.7811 - val_loss: 0.4602\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8541 - loss: 0.3584 - val_accuracy: 0.7842 - val_loss: 0.4582\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8532 - loss: 0.3546 - val_accuracy: 0.7832 - val_loss: 0.4566\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8577 - loss: 0.3495 - val_accuracy: 0.7842 - val_loss: 0.4568\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8618 - loss: 0.3375 - val_accuracy: 0.7863 - val_loss: 0.4556\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8680 - loss: 0.3356 - val_accuracy: 0.7874 - val_loss: 0.4559\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8723 - loss: 0.3274 - val_accuracy: 0.7884 - val_loss: 0.4563\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8738 - loss: 0.3195 - val_accuracy: 0.7874 - val_loss: 0.4558\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8759 - loss: 0.3143 - val_accuracy: 0.7968 - val_loss: 0.4543\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8770 - loss: 0.3118 - val_accuracy: 0.7947 - val_loss: 0.4530\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8803 - loss: 0.3059 - val_accuracy: 0.8000 - val_loss: 0.4520\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8809 - loss: 0.3003 - val_accuracy: 0.8011 - val_loss: 0.4514\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8919 - loss: 0.2859 - val_accuracy: 0.8000 - val_loss: 0.4534\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8921 - loss: 0.2853 - val_accuracy: 0.7989 - val_loss: 0.4527\n",
      "Epoch 44/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8977 - loss: 0.2813 - val_accuracy: 0.7989 - val_loss: 0.4514\n",
      "Epoch 45/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9020 - loss: 0.2705 - val_accuracy: 0.8011 - val_loss: 0.4524\n",
      "Epoch 46/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9028 - loss: 0.2695 - val_accuracy: 0.8011 - val_loss: 0.4505\n",
      "Epoch 47/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9030 - loss: 0.2640 - val_accuracy: 0.8011 - val_loss: 0.4532\n",
      "Epoch 48/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9021 - loss: 0.2595 - val_accuracy: 0.8053 - val_loss: 0.4522\n",
      "Epoch 49/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9082 - loss: 0.2530 - val_accuracy: 0.8042 - val_loss: 0.4545\n",
      "Epoch 50/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9089 - loss: 0.2519 - val_accuracy: 0.8042 - val_loss: 0.4564\n",
      "Epoch 51/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9157 - loss: 0.2436 - val_accuracy: 0.8084 - val_loss: 0.4548\n",
      "Epoch 52/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9189 - loss: 0.2344 - val_accuracy: 0.8074 - val_loss: 0.4566\n",
      "Epoch 53/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9223 - loss: 0.2285 - val_accuracy: 0.8063 - val_loss: 0.4554\n",
      "Epoch 54/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9228 - loss: 0.2248 - val_accuracy: 0.8011 - val_loss: 0.4579\n",
      "Epoch 55/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9212 - loss: 0.2202 - val_accuracy: 0.7989 - val_loss: 0.4602\n",
      "Epoch 56/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9293 - loss: 0.2137 - val_accuracy: 0.8032 - val_loss: 0.4589\n",
      "Epoch 57/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9309 - loss: 0.2101 - val_accuracy: 0.8032 - val_loss: 0.4617\n",
      "Epoch 58/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9322 - loss: 0.2063 - val_accuracy: 0.8011 - val_loss: 0.4630\n",
      "Epoch 59/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9345 - loss: 0.1994 - val_accuracy: 0.8063 - val_loss: 0.4667\n",
      "Epoch 60/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9320 - loss: 0.2001 - val_accuracy: 0.8042 - val_loss: 0.4660\n",
      "Epoch 61/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9409 - loss: 0.1868 - val_accuracy: 0.8095 - val_loss: 0.4680\n",
      "Epoch 62/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9393 - loss: 0.1860 - val_accuracy: 0.8063 - val_loss: 0.4708\n",
      "Epoch 63/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9463 - loss: 0.1760 - val_accuracy: 0.8095 - val_loss: 0.4719\n",
      "Epoch 64/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9469 - loss: 0.1733 - val_accuracy: 0.8105 - val_loss: 0.4784\n",
      "Epoch 65/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9451 - loss: 0.1725 - val_accuracy: 0.8021 - val_loss: 0.4796\n",
      "Epoch 66/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9486 - loss: 0.1650 - val_accuracy: 0.8095 - val_loss: 0.4780\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Accuracy: 0.8010526315789473\n",
      "MCC: 0.6020346202772137\n",
      "AUC: 0.8012691843114964\n",
      "Precision: 0.7837259100642399\n",
      "Recall: 0.8061674008810573\n",
      "Specificity: 0.7963709677419355\n",
      "F1: 0.7947882736156352\n",
      "Training on fold 9/10...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5498 - loss: 0.6843 - val_accuracy: 0.6632 - val_loss: 0.6398\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6569 - loss: 0.6303 - val_accuracy: 0.6979 - val_loss: 0.6033\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6855 - loss: 0.5976 - val_accuracy: 0.7200 - val_loss: 0.5747\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7069 - loss: 0.5708 - val_accuracy: 0.7347 - val_loss: 0.5499\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7266 - loss: 0.5488 - val_accuracy: 0.7400 - val_loss: 0.5306\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7373 - loss: 0.5325 - val_accuracy: 0.7526 - val_loss: 0.5176\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7475 - loss: 0.5136 - val_accuracy: 0.7558 - val_loss: 0.5084\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7602 - loss: 0.5019 - val_accuracy: 0.7632 - val_loss: 0.5019\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7620 - loss: 0.4951 - val_accuracy: 0.7663 - val_loss: 0.4967\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7684 - loss: 0.4872 - val_accuracy: 0.7684 - val_loss: 0.4922\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7765 - loss: 0.4820 - val_accuracy: 0.7747 - val_loss: 0.4877\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7769 - loss: 0.4728 - val_accuracy: 0.7758 - val_loss: 0.4844\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7870 - loss: 0.4615 - val_accuracy: 0.7747 - val_loss: 0.4808\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7905 - loss: 0.4564 - val_accuracy: 0.7779 - val_loss: 0.4770\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7970 - loss: 0.4477 - val_accuracy: 0.7832 - val_loss: 0.4748\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8010 - loss: 0.4448 - val_accuracy: 0.7853 - val_loss: 0.4707\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8086 - loss: 0.4356 - val_accuracy: 0.7895 - val_loss: 0.4690\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8099 - loss: 0.4313 - val_accuracy: 0.7947 - val_loss: 0.4669\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8186 - loss: 0.4203 - val_accuracy: 0.7947 - val_loss: 0.4646\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8184 - loss: 0.4155 - val_accuracy: 0.7968 - val_loss: 0.4621\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8240 - loss: 0.4082 - val_accuracy: 0.7958 - val_loss: 0.4605\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8237 - loss: 0.4044 - val_accuracy: 0.7937 - val_loss: 0.4596\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8291 - loss: 0.3993 - val_accuracy: 0.7958 - val_loss: 0.4575\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8298 - loss: 0.3932 - val_accuracy: 0.7916 - val_loss: 0.4578\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8380 - loss: 0.3840 - val_accuracy: 0.7937 - val_loss: 0.4550\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8361 - loss: 0.3800 - val_accuracy: 0.7947 - val_loss: 0.4551\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8423 - loss: 0.3735 - val_accuracy: 0.7968 - val_loss: 0.4546\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8442 - loss: 0.3674 - val_accuracy: 0.7968 - val_loss: 0.4535\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8482 - loss: 0.3629 - val_accuracy: 0.7926 - val_loss: 0.4530\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8564 - loss: 0.3541 - val_accuracy: 0.7947 - val_loss: 0.4525\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8526 - loss: 0.3538 - val_accuracy: 0.7926 - val_loss: 0.4512\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8539 - loss: 0.3508 - val_accuracy: 0.7979 - val_loss: 0.4497\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8602 - loss: 0.3441 - val_accuracy: 0.7968 - val_loss: 0.4496\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8640 - loss: 0.3363 - val_accuracy: 0.7947 - val_loss: 0.4502\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8626 - loss: 0.3305 - val_accuracy: 0.7958 - val_loss: 0.4492\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8691 - loss: 0.3266 - val_accuracy: 0.7968 - val_loss: 0.4487\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8780 - loss: 0.3182 - val_accuracy: 0.7979 - val_loss: 0.4484\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8747 - loss: 0.3162 - val_accuracy: 0.8032 - val_loss: 0.4489\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8773 - loss: 0.3091 - val_accuracy: 0.8011 - val_loss: 0.4501\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8818 - loss: 0.3088 - val_accuracy: 0.8000 - val_loss: 0.4495\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8803 - loss: 0.3052 - val_accuracy: 0.8053 - val_loss: 0.4500\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8851 - loss: 0.2933 - val_accuracy: 0.8084 - val_loss: 0.4510\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8931 - loss: 0.2893 - val_accuracy: 0.8000 - val_loss: 0.4527\n",
      "Epoch 44/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8878 - loss: 0.2923 - val_accuracy: 0.8042 - val_loss: 0.4515\n",
      "Epoch 45/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8940 - loss: 0.2816 - val_accuracy: 0.8042 - val_loss: 0.4519\n",
      "Epoch 46/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8989 - loss: 0.2778 - val_accuracy: 0.8053 - val_loss: 0.4527\n",
      "Epoch 47/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9011 - loss: 0.2701 - val_accuracy: 0.8084 - val_loss: 0.4538\n",
      "Epoch 48/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9073 - loss: 0.2624 - val_accuracy: 0.8032 - val_loss: 0.4547\n",
      "Epoch 49/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9059 - loss: 0.2568 - val_accuracy: 0.8042 - val_loss: 0.4563\n",
      "Epoch 50/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9109 - loss: 0.2560 - val_accuracy: 0.8084 - val_loss: 0.4588\n",
      "Epoch 51/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9115 - loss: 0.2498 - val_accuracy: 0.8021 - val_loss: 0.4601\n",
      "Epoch 52/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9167 - loss: 0.2437 - val_accuracy: 0.8032 - val_loss: 0.4612\n",
      "Epoch 53/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9184 - loss: 0.2371 - val_accuracy: 0.8021 - val_loss: 0.4656\n",
      "Epoch 54/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9187 - loss: 0.2355 - val_accuracy: 0.8063 - val_loss: 0.4664\n",
      "Epoch 55/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9221 - loss: 0.2267 - val_accuracy: 0.8000 - val_loss: 0.4686\n",
      "Epoch 56/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9205 - loss: 0.2253 - val_accuracy: 0.8021 - val_loss: 0.4716\n",
      "Epoch 57/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9250 - loss: 0.2219 - val_accuracy: 0.8021 - val_loss: 0.4718\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Accuracy: 0.7978947368421052\n",
      "MCC: 0.5947549200396358\n",
      "AUC: 0.7973774600198178\n",
      "Precision: 0.8076152304609219\n",
      "Recall: 0.8076152304609219\n",
      "Specificity: 0.7871396895787139\n",
      "F1: 0.8076152304609219\n",
      "Training on fold 10/10...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5626 - loss: 0.6782 - val_accuracy: 0.6628 - val_loss: 0.6405\n",
      "Epoch 2/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6541 - loss: 0.6317 - val_accuracy: 0.6913 - val_loss: 0.6060\n",
      "Epoch 3/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6913 - loss: 0.5980 - val_accuracy: 0.7018 - val_loss: 0.5807\n",
      "Epoch 4/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7149 - loss: 0.5714 - val_accuracy: 0.7239 - val_loss: 0.5619\n",
      "Epoch 5/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7357 - loss: 0.5461 - val_accuracy: 0.7271 - val_loss: 0.5474\n",
      "Epoch 6/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7433 - loss: 0.5293 - val_accuracy: 0.7313 - val_loss: 0.5373\n",
      "Epoch 7/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7438 - loss: 0.5175 - val_accuracy: 0.7387 - val_loss: 0.5307\n",
      "Epoch 8/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7539 - loss: 0.5059 - val_accuracy: 0.7408 - val_loss: 0.5255\n",
      "Epoch 9/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7622 - loss: 0.4932 - val_accuracy: 0.7408 - val_loss: 0.5222\n",
      "Epoch 10/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7654 - loss: 0.4848 - val_accuracy: 0.7482 - val_loss: 0.5170\n",
      "Epoch 11/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7690 - loss: 0.4771 - val_accuracy: 0.7471 - val_loss: 0.5141\n",
      "Epoch 12/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7831 - loss: 0.4653 - val_accuracy: 0.7503 - val_loss: 0.5131\n",
      "Epoch 13/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7830 - loss: 0.4584 - val_accuracy: 0.7513 - val_loss: 0.5087\n",
      "Epoch 14/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7909 - loss: 0.4483 - val_accuracy: 0.7503 - val_loss: 0.5065\n",
      "Epoch 15/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7929 - loss: 0.4445 - val_accuracy: 0.7471 - val_loss: 0.5050\n",
      "Epoch 16/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8023 - loss: 0.4356 - val_accuracy: 0.7418 - val_loss: 0.5038\n",
      "Epoch 17/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8041 - loss: 0.4308 - val_accuracy: 0.7439 - val_loss: 0.5044\n",
      "Epoch 18/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8097 - loss: 0.4236 - val_accuracy: 0.7429 - val_loss: 0.5019\n",
      "Epoch 19/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8126 - loss: 0.4157 - val_accuracy: 0.7429 - val_loss: 0.5014\n",
      "Epoch 20/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8126 - loss: 0.4077 - val_accuracy: 0.7418 - val_loss: 0.5000\n",
      "Epoch 21/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8198 - loss: 0.3995 - val_accuracy: 0.7482 - val_loss: 0.4981\n",
      "Epoch 22/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8216 - loss: 0.3937 - val_accuracy: 0.7482 - val_loss: 0.4970\n",
      "Epoch 23/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8354 - loss: 0.3863 - val_accuracy: 0.7492 - val_loss: 0.4961\n",
      "Epoch 24/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8356 - loss: 0.3828 - val_accuracy: 0.7503 - val_loss: 0.4982\n",
      "Epoch 25/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8390 - loss: 0.3790 - val_accuracy: 0.7513 - val_loss: 0.4985\n",
      "Epoch 26/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8375 - loss: 0.3703 - val_accuracy: 0.7492 - val_loss: 0.4971\n",
      "Epoch 27/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8464 - loss: 0.3657 - val_accuracy: 0.7471 - val_loss: 0.4974\n",
      "Epoch 28/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8484 - loss: 0.3598 - val_accuracy: 0.7524 - val_loss: 0.4980\n",
      "Epoch 29/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8489 - loss: 0.3544 - val_accuracy: 0.7534 - val_loss: 0.4974\n",
      "Epoch 30/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8561 - loss: 0.3441 - val_accuracy: 0.7524 - val_loss: 0.4989\n",
      "Epoch 31/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8567 - loss: 0.3401 - val_accuracy: 0.7555 - val_loss: 0.4970\n",
      "Epoch 32/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8609 - loss: 0.3343 - val_accuracy: 0.7566 - val_loss: 0.4985\n",
      "Epoch 33/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8665 - loss: 0.3263 - val_accuracy: 0.7555 - val_loss: 0.4989\n",
      "Epoch 34/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8688 - loss: 0.3223 - val_accuracy: 0.7524 - val_loss: 0.4984\n",
      "Epoch 35/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8685 - loss: 0.3211 - val_accuracy: 0.7566 - val_loss: 0.4993\n",
      "Epoch 36/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8730 - loss: 0.3103 - val_accuracy: 0.7576 - val_loss: 0.5009\n",
      "Epoch 37/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8784 - loss: 0.3120 - val_accuracy: 0.7566 - val_loss: 0.5003\n",
      "Epoch 38/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8835 - loss: 0.2965 - val_accuracy: 0.7587 - val_loss: 0.5008\n",
      "Epoch 39/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8821 - loss: 0.2960 - val_accuracy: 0.7587 - val_loss: 0.5042\n",
      "Epoch 40/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8880 - loss: 0.2885 - val_accuracy: 0.7576 - val_loss: 0.5064\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8857 - loss: 0.2877 - val_accuracy: 0.7576 - val_loss: 0.5053\n",
      "Epoch 42/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8912 - loss: 0.2822 - val_accuracy: 0.7619 - val_loss: 0.5073\n",
      "Epoch 43/1000\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8954 - loss: 0.2754 - val_accuracy: 0.7597 - val_loss: 0.5050\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Accuracy: 0.7492096944151738\n",
      "MCC: 0.5010812288124683\n",
      "AUC: 0.7506075090792566\n",
      "Precision: 0.718052738336714\n",
      "Recall: 0.7814569536423841\n",
      "Specificity: 0.719758064516129\n",
      "F1: 0.7484143763213531\n",
      "Cross-validation results (averaged over folds):\n",
      "Accuracy: 0.7779 (+/- 0.0164)\n",
      "Mcc: 0.5573 (+/- 0.0321)\n",
      "Auc: 0.7781 (+/- 0.0162)\n",
      "Precision: 0.7608 (+/- 0.0264)\n",
      "Recall: 0.8112 (+/- 0.0183)\n",
      "Specificity: 0.7449 (+/- 0.0288)\n",
      "F1: 0.7849 (+/- 0.0163)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your input data is X_train (Conv1D input) and X_train_pt5 (ANN input)\n",
    "n_splits = 10  # Set the number of splits for cross-validation\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)  # Deterministic splitting\n",
    "\n",
    "# Placeholder for cross-validation results\n",
    "cv_scores = {\n",
    "    \"accuracy\": [],\n",
    "    \"mcc\": [],\n",
    "    \"auc\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"specificity\": [],\n",
    "    \"f1\": []\n",
    "}\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Training on fold {fold+1}/{n_splits}...\")\n",
    "    \n",
    "    # Split the data based on the KFold indices\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    X_train_pt5_fold, X_val_pt5_fold = X_train_pt5[train_idx], X_train_pt5[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    # Rebuild the model for each fold (important for cross-validation)\n",
    "    inception_model = build_inception_model((33,), 1024)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model on the current fold\n",
    "    history = inception_model.fit([X_train_fold, X_train_pt5_fold], y_train_fold,\n",
    "                               epochs=1000, batch_size=64, verbose=1,\n",
    "                               validation_data=([X_val_fold, X_val_pt5_fold], y_val_fold), callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the validation data and record the score\n",
    "    scores = evaluate_model(inception_model, X_val_fold, X_val_pt5_fold, y_val_fold)\n",
    "    \n",
    "    # Append scores for each metric\n",
    "    cv_scores[\"accuracy\"].append(scores[0])\n",
    "    cv_scores[\"mcc\"].append(scores[1])\n",
    "    cv_scores[\"auc\"].append(scores[2])\n",
    "    cv_scores[\"precision\"].append(scores[3])\n",
    "    cv_scores[\"recall\"].append(scores[4])\n",
    "    cv_scores[\"specificity\"].append(scores[5])\n",
    "    cv_scores[\"f1\"].append(scores[6])\n",
    "\n",
    "# Print average metrics after cross-validation\n",
    "print(\"Cross-validation results (averaged over folds):\")\n",
    "for metric, values in cv_scores.items():\n",
    "    print(f\"{metric.capitalize()}: {np.mean(values):.4f} (+/- {np.std(values):.4f})\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "inception_model.save('models/inception_model_full_train.h5')\n",
    "inception_model.save_weights('models/inception_model_full_train.weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
