{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3085\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# read every csv file from a folder and concatenate them into a single dataframe with a new column for the filename as protein_id\n",
    "folder = 'proba/'\n",
    "dfs = []\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.read_csv(folder + file)\n",
    "        df.columns = ['site', 'probability']\n",
    "        df['protein_id'] = file.split('.')[0]\n",
    "        dfs.append(df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "test = pd.read_csv('../../Embeddings/Prot_t5/test_t5.csv')\n",
    "\n",
    "# match the protein_id , site and add label from test to the dataframe\n",
    "df['site'] = df['site'].astype(str)\n",
    "df['site'] = df['site'].str.replace('site_', '')\n",
    "df['site'] = df['site'].astype(int)\n",
    "df = df.merge(test, on=['protein_id', 'site'], how='left')\n",
    "\n",
    "df.drop(columns=['embedding', 'sequence'], inplace=True)\n",
    "\n",
    "df.dropna(subset=['label'], inplace=True)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df.to_csv('proba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, average_precision_score\n",
    "import numpy as np\n",
    "def evaluate_model(y_pred_probs, y_true, print_metrics=True):\n",
    "\n",
    "    # Convert probabilities/logits to binary predictions (threshold = 0.5).\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # If y_true is one-hot encoded, convert it to binary format\n",
    "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:  # Check if y_true is one-hot encoded\n",
    "        y_true = np.argmax(y_true, axis=1)  # Convert one-hot encoded y_true to binary labels\n",
    "\n",
    "    # Ensure y_pred is also 1D\n",
    "    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)  # Convert y_pred to binary labels if necessary\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    auprc = average_precision_score(y_true, y_pred_probs)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Compute Specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Print the results\n",
    "    if print_metrics:\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        print(f'MCC: {mcc:.4f}')\n",
    "        print(f'AUC: {auc:.4f}')\n",
    "        print(f'AUPRC: {auprc:.4f}')\n",
    "        print(f'Precision: {precision:.4f}')\n",
    "        print(f'Recall: {recall:.4f}')\n",
    "        print(f'Specificity: {specificity:.4f}')\n",
    "        print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    return accuracy, mcc, auc, auprc, precision, recall, specificity, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7446\n",
      "MCC: 0.3088\n",
      "AUC: 0.8357\n",
      "AUPRC: 0.3077\n",
      "Precision: 0.2041\n",
      "Recall: 0.7875\n",
      "Specificity: 0.7409\n",
      "F1 Score: 0.3242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7445705024311183,\n",
       " 0.3088289817877136,\n",
       " 0.8357322788517867,\n",
       " 0.30769136881825615,\n",
       " 0.2041036717062635,\n",
       " 0.7875,\n",
       " 0.7409490333919156,\n",
       " 0.3241852487135506)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pSuc_df = pd.read_csv('proba.csv')\n",
    "y_prob_pSuc = pSuc_df['probability'].values\n",
    "y_true_pSuc = pSuc_df['label'].values\n",
    "\n",
    "evaluate_model(y_prob_pSuc, y_true_pSuc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
